{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automative_data_cleaning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries**"
      ],
      "metadata": {
        "id": "8oFQWCA2QJCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKfoCb_yEHfU",
        "outputId": "c9e75121-b8a0-4c42-fec0-3793eaf90ccf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Challenge Business & AI/B&AI/EDA') \n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "gZR6t6OgEPVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "renault_cluster = pd.read_csv(\"/content/drive/MyDrive/Challenge Business & AI/B&AI/EDA/ads_cluster_1.csv\", low_memory = False, sep=\";\")"
      ],
      "metadata": {
        "id": "TB2y1KfVGzDV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning Modules**"
      ],
      "metadata": {
        "id": "1PwZ3ux7QLTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import isnan\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from loguru import logger\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "'''\n",
        "Modules are used by the automative cleaning pipeline for data cleaning and preprocessing.\n",
        "'''\n",
        "\n",
        "class MissingValues:\n",
        "\n",
        "    def handle(self, df, _n_neighbors=3):\n",
        "        # function for handling missing values in the data\n",
        "        if self.missing_num or self.missing_categ:\n",
        "            logger.info('Started handling of missing values...', str(self.missing_num).upper())\n",
        "            start = timer()\n",
        "            self.count_missing = df.isna().sum().sum()\n",
        "\n",
        "            if self.count_missing != 0:\n",
        "                logger.info('Found a total of {} missing value(s)', self.count_missing)\n",
        "                df = df.dropna(how='all')\n",
        "                df.reset_index(drop=True)\n",
        "                \n",
        "                if self.missing_num: # numeric data\n",
        "                    logger.info('Started handling of NUMERICAL missing values... Method: \"{}\"', str(self.missing_num).upper())\n",
        "                    # automated handling\n",
        "                    if self.missing_num == 'auto': \n",
        "                        self.missing_num = 'linreg'\n",
        "                        lr = LinearRegression()\n",
        "                        df = MissingValues._lin_regression_impute(self, df, lr)\n",
        "                        self.missing_num = 'knn'\n",
        "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
        "                        df = MissingValues._impute(self, df, imputer, type='num')\n",
        "                    # linear regression imputation\n",
        "                    elif self.missing_num == 'linreg':\n",
        "                        lr = LinearRegression()\n",
        "                        df = MissingValues._lin_regression_impute(self, df, lr)\n",
        "                    # knn imputation\n",
        "                    elif self.missing_num == 'knn':\n",
        "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
        "                        df = MissingValues._impute(self, df, imputer, type='num')\n",
        "                    # mean, median or mode imputation\n",
        "                    elif self.missing_num in ['mean', 'median', 'most_frequent']:\n",
        "                        imputer = SimpleImputer(strategy=self.missing_num)\n",
        "                        df = MissingValues._impute_missing(self, df, imputer, type='num')\n",
        "                    # delete missing values\n",
        "                    elif self.missing_num == 'delete':\n",
        "                        df = MissingValues._delete(self, df, type='num')\n",
        "                        logger.debug('Deletion of {} NUMERIC missing value(s) succeeded', self.count_missing-df.isna().sum().sum())      \n",
        "\n",
        "                if self.missing_categ: # categorical data\n",
        "                    logger.info('Started handling of CATEGORICAL missing values... Method: \"{}\"', str(self.missing_categ).upper())\n",
        "                    # automated handling\n",
        "                    if self.missing_categ == 'auto':\n",
        "                        self.missing_categ = 'logreg'\n",
        "                        lr = LogisticRegression()\n",
        "                        df = MissingValues._log_regression_impute(self, df, lr)\n",
        "                        self.missing_categ = 'knn'\n",
        "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
        "                        df = MissingValues._impute(self, df, imputer, type='categ')\n",
        "                    elif self.missing_categ == 'logreg':\n",
        "                        lr = LogisticRegression()\n",
        "                        df = MissingValues._log_regression_impute(self, df, lr)\n",
        "                    # knn imputation\n",
        "                    elif self.missing_categ == 'knn':\n",
        "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
        "                        df = MissingValues._impute(self, df, imputer, type='categ')  \n",
        "                    # mode imputation\n",
        "                    elif self.missing_categ == 'most_frequent':\n",
        "                        imputer = SimpleImputer(strategy=self.missing_categ)\n",
        "                        df = MissingValues._impute(self, df, imputer, type='categ')\n",
        "                    # delete missing values                    \n",
        "                    elif self.missing_categ == 'delete':\n",
        "                        df = MissingValues._delete(self, df, type='categ')\n",
        "                        logger.debug('Deletion of {} CATEGORICAL missing value(s) succeeded', self.count_missing-df.isna().sum().sum())\n",
        "            else:\n",
        "                logger.debug('{} missing values found', self.count_missing)\n",
        "            end = timer()\n",
        "            logger.info('Completed handling of missing values in {} seconds', round(end-start, 6))  \n",
        "        else:\n",
        "            logger.info('Skipped handling of missing values')\n",
        "        return df\n",
        "\n",
        "    def _impute(self, df, imputer, type):\n",
        "        # function for imputing missing values in the data\n",
        "        cols_num = df.select_dtypes(include=np.number).columns \n",
        "\n",
        "        if type == 'num':\n",
        "            # numerical features\n",
        "            for feature in df.columns: \n",
        "                if feature in cols_num:\n",
        "                    if df[feature].isna().sum().sum() != 0:\n",
        "                        try:\n",
        "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))\n",
        "                            counter = df[feature].isna().sum().sum() - df_imputed.isna().sum().sum()\n",
        "\n",
        "                            if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                                df[feature] = df_imputed\n",
        "                                # round back to INTs, if original data were INTs\n",
        "                                df[feature] = df[feature].round()\n",
        "                                df[feature] = df[feature].astype('Int64')                                        \n",
        "                            else:\n",
        "                                df[feature] = df_imputed\n",
        "                            if counter != 0:\n",
        "                                logger.debug('{} imputation of {} value(s) succeeded for feature \"{}\"', str(self.missing_num).upper(), counter, feature)\n",
        "                        except:\n",
        "                            logger.warning('{} imputation failed for feature \"{}\"', str(self.missing_num).upper(), feature)\n",
        "        else:\n",
        "            # categorical features\n",
        "            for feature in df.columns:\n",
        "                if feature not in cols_num:\n",
        "                    if df[feature].isna().sum()!= 0:\n",
        "                        try:\n",
        "                            mapping = dict()\n",
        "                            mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}\n",
        "                            mapping[feature] = mappings\n",
        "                            df[feature] = df[feature].map(mapping[feature])\n",
        "\n",
        "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])    \n",
        "                            counter = sum(1 for i, j in zip(list(df_imputed[feature]), list(df[feature])) if i != j)\n",
        "\n",
        "                            # round to integers before mapping back to original values\n",
        "                            df[feature] = df_imputed\n",
        "                            df[feature] = df[feature].round()\n",
        "                            df[feature] = df[feature].astype('Int64')  \n",
        "\n",
        "                            # map values back to original\n",
        "                            mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
        "                            df[feature] = df[feature].map(mappings_inv)\n",
        "                            if counter != 0:\n",
        "                                logger.debug('{} imputation of {} value(s) succeeded for feature \"{}\"', self.missing_categ.upper(), counter, feature)\n",
        "                        except:\n",
        "                            logger.warning('{} imputation failed for feature \"{}\"', str(self.missing_categ).upper(), feature)\n",
        "        return df\n",
        "\n",
        "    def _lin_regression_impute(self, df, model):\n",
        "        # function for predicting missing values with linear regression\n",
        "        cols_num = df.select_dtypes(include=np.number).columns\n",
        "        mapping = dict()\n",
        "        for feature in df.columns:\n",
        "            if feature not in cols_num:\n",
        "                # create label mapping for categorical feature values\n",
        "                mappings = {k: i for i, k in enumerate(df[feature])}\n",
        "                mapping[feature] = mappings\n",
        "                df[feature] = df[feature].map(mapping[feature])\n",
        "        for feature in cols_num: \n",
        "                try:\n",
        "                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
        "                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
        "                    if len(test_df.index) != 0:\n",
        "                        pipe = make_pipeline(StandardScaler(), model)\n",
        "\n",
        "                        y = np.log(train_df[feature]) # log-transform the data\n",
        "                        X_train = train_df.drop(feature, axis=1)\n",
        "                        test_df.drop(feature, axis=1, inplace=True)\n",
        "                        \n",
        "                        try:\n",
        "                            model = pipe.fit(X_train, y)\n",
        "                        except:\n",
        "                            y = train_df[feature] # use non-log-transformed data\n",
        "                            model = pipe.fit(X_train, y)\n",
        "                        if (y == train_df[feature]).all():\n",
        "                            pred = model.predict(test_df)\n",
        "                        else:\n",
        "                            pred = np.exp(model.predict(test_df)) # predict values\n",
        "\n",
        "                        test_df[feature]= pred\n",
        "\n",
        "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                            # round back to INTs, if original data were INTs\n",
        "                            test_df[feature] = test_df[feature].round()\n",
        "                            test_df[feature] = test_df[feature].astype('Int64')\n",
        "                            df[feature].update(test_df[feature])                          \n",
        "                        else:\n",
        "                            df[feature].update(test_df[feature])  \n",
        "                        logger.debug('LINREG imputation of {} value(s) succeeded for feature \"{}\"', len(pred), feature)\n",
        "                except:\n",
        "                    logger.warning('LINREG imputation failed for feature \"{}\"', feature)\n",
        "        for feature in df.columns: \n",
        "            try:   \n",
        "                # map categorical feature values back to original\n",
        "                mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
        "                df[feature] = df[feature].map(mappings_inv)\n",
        "            except:\n",
        "                pass\n",
        "        return df\n",
        "\n",
        "    def _log_regression_impute(self, df, model):\n",
        "        # function for predicting missing values with logistic regression\n",
        "        cols_num = df.select_dtypes(include=np.number).columns\n",
        "        mapping = dict()\n",
        "        for feature in df.columns:\n",
        "            if feature not in cols_num:\n",
        "                # create label mapping for categorical feature values\n",
        "                mappings = {k: i for i, k in enumerate(df[feature])} #.dropna().unique(), 0)}\n",
        "                mapping[feature] = mappings\n",
        "                df[feature] = df[feature].map(mapping[feature])\n",
        "\n",
        "        target_cols = [x for x in df.columns if x not in cols_num]\n",
        "            \n",
        "        for feature in df.columns: \n",
        "            if feature in target_cols:\n",
        "                try:\n",
        "                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
        "                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
        "                    if len(test_df.index) != 0:\n",
        "                        pipe = make_pipeline(StandardScaler(), model)\n",
        "\n",
        "                        y = train_df[feature]\n",
        "                        train_df.drop(feature, axis=1, inplace=True)\n",
        "                        test_df.drop(feature, axis=1, inplace=True)\n",
        "\n",
        "                        model = pipe.fit(train_df, y)\n",
        "                        \n",
        "                        pred = model.predict(test_df) # predict values\n",
        "                        test_df[feature]= pred\n",
        "\n",
        "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                            # round back to INTs, if original data were INTs\n",
        "                            test_df[feature] = test_df[feature].round()\n",
        "                            test_df[feature] = test_df[feature].astype('Int64')\n",
        "                            df[feature].update(test_df[feature])                             \n",
        "                        logger.debug('LOGREG imputation of {} value(s) succeeded for feature \"{}\"', len(pred), feature)\n",
        "                except:\n",
        "                    logger.warning('LOGREG imputation failed for feature \"{}\"', feature)\n",
        "        for feature in df.columns: \n",
        "            try:\n",
        "                # map categorical feature values back to original\n",
        "                mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
        "                df[feature] = df[feature].map(mappings_inv)\n",
        "            except:\n",
        "                pass     \n",
        "        return df\n",
        "\n",
        "    def _delete(self, df, type):\n",
        "        # function for deleting missing values\n",
        "        cols_num = df.select_dtypes(include=np.number).columns \n",
        "        if type == 'num':\n",
        "            # numerical features\n",
        "            for feature in df.columns: \n",
        "                if feature in cols_num:\n",
        "                    df = df.dropna(subset=[feature])\n",
        "                    df.reset_index(drop=True)\n",
        "        else:\n",
        "            # categorical features\n",
        "            for feature in df.columns:\n",
        "                if feature not in cols_num:\n",
        "                    df = df.dropna(subset=[feature])\n",
        "                    df.reset_index(drop=True)\n",
        "        return df                    \n",
        "\n",
        "class Outliers:\n",
        "\n",
        "    def handle(self, df):\n",
        "        # function for handling of outliers in the data\n",
        "        if self.outliers:\n",
        "            logger.info('Started handling of outliers... Method: \"{}\"', str(self.outliers).upper())\n",
        "            start = timer()  \n",
        "\n",
        "            if self.outliers in ['auto', 'winz']:  \n",
        "                df = Outliers._winsorization(self, df)\n",
        "            elif self.outliers == 'delete':\n",
        "                df = Outliers._delete(self, df)\n",
        "            \n",
        "            end = timer()\n",
        "            logger.info('Completed handling of outliers in {} seconds', round(end-start, 6))\n",
        "        else:\n",
        "            logger.info('Skipped handling of outliers')\n",
        "        return df     \n",
        "\n",
        "    def _winsorization(self, df):\n",
        "        # function for outlier winsorization\n",
        "        cols_num = df.select_dtypes(include=np.number).columns    \n",
        "        for feature in cols_num:           \n",
        "            counter = 0\n",
        "            # compute outlier bounds\n",
        "            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)    \n",
        "            for row_index, row_val in enumerate(df[feature]):\n",
        "                if row_val < lower_bound or row_val > upper_bound:\n",
        "                    if row_val < lower_bound:\n",
        "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                                df.loc[row_index, feature] = lower_bound\n",
        "                                df[feature] = df[feature].astype(int) \n",
        "                        else:    \n",
        "                            df.loc[row_index, feature] = lower_bound\n",
        "                        counter += 1\n",
        "                    else:\n",
        "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                            df.loc[row_index, feature] = upper_bound\n",
        "                            df[feature] = df[feature].astype(int) \n",
        "                        else:\n",
        "                            df.loc[row_index, feature] = upper_bound\n",
        "                        counter += 1\n",
        "            if counter != 0:\n",
        "                logger.debug('Outlier imputation of {} value(s) succeeded for feature \"{}\"', counter, feature)        \n",
        "        return df\n",
        "\n",
        "    def _delete(self, df):\n",
        "        # function for deleting outliers in the data\n",
        "        cols_num = df.select_dtypes(include=np.number).columns    \n",
        "        for feature in cols_num:\n",
        "            counter = 0\n",
        "            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)    \n",
        "            # delete observations containing outliers            \n",
        "            for row_index, row_val in enumerate(df[feature]):\n",
        "                if row_val < lower_bound or row_val > upper_bound:\n",
        "                    df = df.drop(row_index)\n",
        "                    counter +=1\n",
        "            df = df.reset_index(drop=True)\n",
        "            if counter != 0:\n",
        "                logger.debug('Deletion of {} outliers succeeded for feature \"{}\"', counter, feature)\n",
        "        return df\n",
        "\n",
        "    def _compute_bounds(self, df, feature):\n",
        "        # function that computes the lower and upper bounds for finding outliers in the data\n",
        "        featureSorted = sorted(df[feature])\n",
        "        \n",
        "        q1, q3 = np.percentile(featureSorted, [25, 75])\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        lb = q1 - (self.outlier_param * iqr) \n",
        "        ub = q3 + (self.outlier_param * iqr) \n",
        "\n",
        "        return lb, ub    \n",
        "\n",
        "class Adjust:\n",
        "\n",
        "    def convert_datetime(self, df):\n",
        "        # function for extracting of datetime values in the data\n",
        "        if self.extract_datetime:\n",
        "            logger.info('Started conversion of DATETIME features... Granularity: {}', self.extract_datetime)\n",
        "            start = timer()\n",
        "            cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
        "            for feature in cols: \n",
        "                try:\n",
        "                    # convert features encoded as strings to type datetime ['D','M','Y','h','m','s']\n",
        "                    df[feature] = pd.to_datetime(df[feature], infer_datetime_format=True)\n",
        "                    try:\n",
        "                        df['Day'] = pd.to_datetime(df[feature]).dt.day\n",
        "\n",
        "                        if self.extract_datetime in ['auto', 'M','Y','h','m','s']:\n",
        "                            df['Month'] = pd.to_datetime(df[feature]).dt.month\n",
        "\n",
        "                            if self.extract_datetime in ['auto', 'Y','h','m','s']:\n",
        "                                df['Year'] = pd.to_datetime(df[feature]).dt.year\n",
        "\n",
        "                                if self.extract_datetime in ['auto', 'h','m','s']:\n",
        "                                    df['Hour'] = pd.to_datetime(df[feature]).dt.hour\n",
        "\n",
        "                                    if self.extract_datetime in ['auto', 'm','s']:\n",
        "                                        df['Minute'] = pd.to_datetime(df[feature]).dt.minute\n",
        "\n",
        "                                        if self.extract_datetime in ['auto', 's']:\n",
        "                                            df['Sec'] = pd.to_datetime(df[feature]).dt.second\n",
        "                        \n",
        "                        logger.debug('Conversion to DATETIME succeeded for feature \"{}\"', feature)\n",
        "\n",
        "                        try: \n",
        "                            # check if entries for the extracted dates/times are non-NULL, otherwise drop\n",
        "                            if (df['Hour'] == 0).all() and (df['Minute'] == 0).all() and (df['Sec'] == 0).all():\n",
        "                                df.drop('Hour', inplace = True, axis =1 )\n",
        "                                df.drop('Minute', inplace = True, axis =1 )\n",
        "                                df.drop('Sec', inplace = True, axis =1 )\n",
        "                            elif (df['Day'] == 0).all() and (df['Month'] == 0).all() and (df['Year'] == 0).all():\n",
        "                                df.drop('Day', inplace = True, axis =1 )\n",
        "                                df.drop('Month', inplace = True, axis =1 )\n",
        "                                df.drop('Year', inplace = True, axis =1 )   \n",
        "                        except:\n",
        "                            pass          \n",
        "                    except:\n",
        "                        # feature cannot be converted to datetime\n",
        "                        logger.warning('Conversion to DATETIME failed for \"{}\"', feature)\n",
        "                except:\n",
        "                    pass\n",
        "            end = timer()\n",
        "            logger.info('Completed conversion of DATETIME features in {} seconds', round(end-start, 4))\n",
        "        else:\n",
        "            logger.info('Skipped datetime feature conversion')\n",
        "        return df\n",
        "\n",
        "    def round_values(self, df, input_data):\n",
        "        # function that checks datatypes of features and converts them if necessary\n",
        "        if self.duplicates or self.missing_num or self.missing_categ or self.outliers or self.encode_categ or self.extract_datetime:\n",
        "            logger.info('Started feature type conversion...')\n",
        "            start = timer()\n",
        "            counter = 0\n",
        "            cols_num = df.select_dtypes(include=np.number).columns\n",
        "            for feature in cols_num:\n",
        "                    # check if all values are integers\n",
        "                    if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                        try:\n",
        "                            # encode FLOATs with only 0 as decimals to INT\n",
        "                            df[feature] = df[feature].astype('Int64')\n",
        "                            counter += 1\n",
        "                            logger.debug('Conversion to type INT succeeded for feature \"{}\"', feature)\n",
        "                        except:\n",
        "                            logger.warning('Conversion to type INT failed for feature \"{}\"', feature)\n",
        "                    else:\n",
        "                        try:\n",
        "                            df[feature] = df[feature].astype(float)\n",
        "                            # round the number of decimals of FLOATs back to original\n",
        "                            dec = None\n",
        "                            for value in input_data[feature]:\n",
        "                                try:\n",
        "                                    if dec == None:\n",
        "                                        dec = str(value)[::-1].find('.')\n",
        "                                    else:\n",
        "                                        if str(value)[::-1].find('.') > dec:\n",
        "                                            dec = str(value)[::-1].find('.')\n",
        "                                except:\n",
        "                                    pass\n",
        "                            df[feature] = df[feature].round(decimals = dec)\n",
        "                            counter += 1\n",
        "                            logger.debug('Conversion to type FLOAT succeeded for feature \"{}\"', feature)\n",
        "                        except:\n",
        "                            logger.warning('Conversion to type FLOAT failed for feature \"{}\"', feature)\n",
        "            end = timer()\n",
        "            logger.info('Completed feature type conversion for {} feature(s) in {} seconds', counter, round(end-start, 6))\n",
        "        else:\n",
        "            logger.info('Skipped feature type conversion')\n",
        "        return df\n",
        "\n",
        "class EncodeCateg:\n",
        "\n",
        "    def handle(self, df):\n",
        "        # function for encoding of categorical features in the data\n",
        "        if self.encode_categ:\n",
        "            if not isinstance(self.encode_categ, list):\n",
        "                self.encode_categ = ['auto']\n",
        "            # select non numeric features\n",
        "            cols_categ = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
        "            # check if all columns should be encoded\n",
        "            if len(self.encode_categ) == 1:\n",
        "                target_cols = cols_categ # encode ALL columns\n",
        "            else:\n",
        "                target_cols = self.encode_categ[1] # encode only specific columns\n",
        "            logger.info('Started encoding categorical features... Method: \"{}\"', str(self.encode_categ[0]).upper())\n",
        "            start = timer()\n",
        "            for feature in target_cols:\n",
        "                if feature in cols_categ:\n",
        "                    # columns are column names\n",
        "                    feature = feature\n",
        "                else:\n",
        "                    # columns are indexes\n",
        "                    feature = df.columns[feature]\n",
        "                try:\n",
        "                    # skip encoding of datetime features\n",
        "                    pd.to_datetime(df[feature])\n",
        "                    logger.debug('Skipped encoding for DATETIME feature \"{}\"', feature)\n",
        "                except:\n",
        "                    try:\n",
        "                        if self.encode_categ[0] == 'auto':\n",
        "                            # ONEHOT encode if not more than 10 unique values to encode\n",
        "                            if df[feature].nunique() <=10:\n",
        "                                df = EncodeCateg._to_onehot(self, df, feature)\n",
        "                                logger.debug('Encoding to ONEHOT succeeded for feature \"{}\"', feature)\n",
        "                            # LABEL encode if not more than 20 unique values to encode\n",
        "                            elif df[feature].nunique() <=20:\n",
        "                                df = EncodeCateg._to_label(self, df, feature)\n",
        "                                logger.debug('Encoding to LABEL succeeded for feature \"{}\"', feature)\n",
        "                            # skip encoding if more than 20 unique values to encode\n",
        "                            else:\n",
        "                                logger.debug('Encoding skipped for feature \"{}\"', feature)   \n",
        "\n",
        "                        elif self.encode_categ[0] == 'onehot':\n",
        "                            df = EncodeCateg._to_onehot(df, feature)\n",
        "                            logger.debug('Encoding to {} succeeded for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)\n",
        "                        elif self.encode_categ[0] == 'label':\n",
        "                            df = EncodeCateg._to_label(df, feature)\n",
        "                            logger.debug('Encoding to {} succeeded for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)      \n",
        "                    except:\n",
        "                        logger.warning('Encoding to {} failed for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)    \n",
        "            end = timer()\n",
        "            logger.info('Completed encoding of categorical features in {} seconds', round(end-start, 6))\n",
        "        else:\n",
        "            logger.info('Skipped encoding of categorical features')\n",
        "        return df\n",
        "\n",
        "    def _to_onehot(self, df, feature, limit=10):  \n",
        "        # function that encodes categorical features to OneHot encodings    \n",
        "        one_hot = pd.get_dummies(df[feature], prefix=feature)\n",
        "        if one_hot.shape[1] > limit:\n",
        "            logger.warning('ONEHOT encoding for feature \"{}\" creates {} new features. Consider LABEL encoding instead.', feature, one_hot.shape[1])\n",
        "        # join the encoded df\n",
        "        df = df.join(one_hot)\n",
        "        return df\n",
        "\n",
        "    def _to_label(self, df, feature):\n",
        "        # function that encodes categorical features to label encodings \n",
        "        le = preprocessing.LabelEncoder()\n",
        "\n",
        "        df[feature + '_lab'] = le.fit_transform(df[feature].values)\n",
        "        mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
        "        \n",
        "        for key in mapping:\n",
        "            try:\n",
        "                if isnan(key):               \n",
        "                    replace = {mapping[key] : key }\n",
        "                    df[feature].replace(replace, inplace=True)\n",
        "            except:\n",
        "                pass\n",
        "        return df  \n",
        "\n",
        "class Duplicates:\n",
        "\n",
        "    def handle(self, df):\n",
        "        if self.duplicates:\n",
        "            logger.info('Started handling of duplicates... Method: \"{}\"', str(self.duplicates).upper())\n",
        "            start = timer()\n",
        "            original = df.shape\n",
        "            try:\n",
        "                df.drop_duplicates(inplace=True, ignore_index=False)\n",
        "                df = df.reset_index(drop=True)\n",
        "                new = df.shape\n",
        "                count = original[0] - new[0]\n",
        "                if count != 0:\n",
        "                    logger.debug('Deletion of {} duplicate(s) succeeded', count)\n",
        "                else:\n",
        "                    logger.debug('{} missing values found', count)\n",
        "                end = timer()\n",
        "                logger.info('Completed handling of duplicates in {} seconds', round(end-start, 6))\n",
        "\n",
        "            except:\n",
        "                logger.warning('Handling of duplicates failed')        \n",
        "        else:\n",
        "            logger.info('Skipped handling of duplicates')\n",
        "        return df "
      ],
      "metadata": {
        "id": "KAzURchlHb72"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install loguru"
      ],
      "metadata": {
        "id": "f-7lR1obGLTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cKUuDEfwDV-h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from timeit import default_timer as timer\n",
        "import pandas as pd\n",
        "from loguru import logger\n",
        "\n",
        "class AutoClean:\n",
        "\n",
        "    def __init__(self, input_data, mode='auto', duplicates=False, missing_num=False, missing_categ=False, encode_categ=False, extract_datetime=False, outliers=False, outlier_param=1.5, logfile=True, verbose=False):  \n",
        "        '''\n",
        "        input_data (dataframe)..........Pandas dataframe\n",
        "        mode (str)......................define in which mode you want to run AutoClean\n",
        "                                        'auto' = sets all parameters to 'auto' and let AutoClean do the data cleaning automatically\n",
        "                                        'manual' = lets you choose which parameters/cleaning steps you want to perform\n",
        "                                        \n",
        "        duplicates (str)................define if duplicates in the data should be handled\n",
        "                                        duplicates are rows where all features are identical\n",
        "                                        'auto' = automated handling, deletes all copies of duplicates except one\n",
        "                                        False = skips this step\n",
        "        missing_num (str)...............define how NUMERICAL missing values are handled\n",
        "                                        'auto' = automated handling\n",
        "                                        'linreg' = uses Linear Regression for predicting missing values\n",
        "                                        'knn' = uses K-NN algorithm for imputation\n",
        "                                        'mean','median' or 'most_frequent' = uses mean/median/mode imputatiom\n",
        "                                        'delete' = deletes observations with missing values\n",
        "                                        False = skips this step\n",
        "        missing_categ (str).............define how CATEGORICAL missing values are handled\n",
        "                                        'auto' = automated handling\n",
        "                                        'logreg' = uses Logistic Regression for predicting missing values\n",
        "                                        'knn' = uses K-NN algorithm for imputation\n",
        "                                        'most_frequent' = uses mode imputatiom\n",
        "                                        'delete' = deletes observations with missing values\n",
        "                                        False = skips this step\n",
        "        encode_categ (list).............encode CATEGORICAL features, takes a list as input\n",
        "                                        ['auto'] = automated encoding\n",
        "                                        ['onehot'] = one-hot-encode all CATEGORICAL features\n",
        "                                        ['label'] = label-encode all categ. features\n",
        "                                        to encode only specific features add the column name or index: ['onehot', ['col1', 2]]\n",
        "                                        False = skips this step\n",
        "        extract_datetime (str)..........define whether DATETIME type features should be extracted into separate features\n",
        "                                        to define granularity set to 'D'= day, 'M'= month, 'Y'= year, 'h'= hour, 'm'= minute or 's'= second\n",
        "                                        False = skips this step\n",
        "        outliers (str)..................define how outliers are handled\n",
        "                                        'winz' = replaces outliers through winsorization\n",
        "                                        'delete' = deletes observations containing outliers\n",
        "                                        oberservations are considered outliers if they are outside the lower and upper bound [Q1-1.5*IQR, Q3+1.5*IQR], where IQR is the interquartile range\n",
        "                                        to set a custom multiplier use the 'outlier_param' parameter\n",
        "                                        False = skips this step\n",
        "        outlier_param (int, float)......define the multiplier for the outlier bounds\n",
        "        logfile (bool)..................define whether to create a logile during the AutoClean process\n",
        "                                        logfile will be saved in working directory as \"autoclean.log\"\n",
        "        verbose (bool)..................define whether AutoClean logs will be printed in console\n",
        "        \n",
        "        OUTPUT (dataframe)..............a cleaned Pandas dataframe, accessible through the 'output' instance\n",
        "        '''\n",
        "        start = timer()\n",
        "        self._initialize_logger(verbose, logfile)\n",
        "        \n",
        "        output_data = input_data.copy()\n",
        "\n",
        "        if mode == 'auto':\n",
        "            duplicates, missing_num, missing_categ, outliers, encode_categ, extract_datetime = 'auto', 'auto', 'auto', 'winz', ['auto'], 's'\n",
        "\n",
        "        self.mode = mode\n",
        "        self.duplicates = duplicates\n",
        "        self.missing_num = missing_num\n",
        "        self.missing_categ = missing_categ\n",
        "        self.outliers = outliers\n",
        "        self.encode_categ = encode_categ\n",
        "        self.extract_datetime = extract_datetime\n",
        "        self.outlier_param = outlier_param\n",
        "        \n",
        "        # validate the input parameters\n",
        "        self._validate_params(output_data, verbose, logfile)\n",
        "        \n",
        "        # initialize our class and start the autoclean process\n",
        "        self.output = self._clean_data(output_data, input_data)  \n",
        "\n",
        "        end = timer()\n",
        "        logger.info('AutoClean process completed in {} seconds', round(end-start, 6))\n",
        "\n",
        "        if not verbose:\n",
        "            print('AutoClean process completed in', round(end-start, 6), 'seconds')\n",
        "        if logfile:\n",
        "            print('Logfile saved to:', os.path.join(os.getcwd(), 'autoclean.log'))\n",
        "\n",
        "    def help():\n",
        "        # function that outputs some basic usage information \n",
        "        help_msg = f\"\"\"\n",
        "        **** Welcome to AutoClean! {__version__} ****\n",
        "        Run AutoClean by selecting your input data (Pandas dataframe) and setting the 'mode' parameter to:\n",
        "        \\t* 'auto' (default) or\n",
        "        \\t* 'manual'\n",
        "        If set to 'auto', AutoClean will start the automated cleaning process. \n",
        "        If set to 'manual', you can customize your AutoClean pipeline by defining some of the optional parameters:\n",
        "        \\tduplicates, missing_num, missing_categ, outliers, encode_categ, extract_datetime\n",
        "        ðŸ“‹ For detailed documentation and usage guide, please visit the official GitHub Repo: https://github.com/elisemercury/AutoClean\n",
        "        \"\"\"     \n",
        "        print(help_msg)\n",
        "        return\n",
        "\n",
        "    def _initialize_logger(self, verbose, logfile):\n",
        "        # function for initializing the logging process\n",
        "        logger.remove()\n",
        "        if verbose == True:\n",
        "            logger.add(sys.stderr, format='{time:DD-MM-YYYY HH:mm:ss.SS} - {level} - {message}')\n",
        "        if logfile == True:    \n",
        "            logger.add('autoclean.log', mode='w', format='{time:DD-MM-YYYY HH:mm:ss.SS} - {level} - {message}')\n",
        "        return\n",
        "\n",
        "    def _validate_params(self, df, verbose, logfile):\n",
        "        # function for validating the input parameters of the autolean process\n",
        "        logger.info('Started validation of input parameters...')\n",
        "        \n",
        "        if type(df) != pd.core.frame.DataFrame:\n",
        "            raise ValueError('Invalid value for \"df\" parameter.')\n",
        "        if self.mode not in ['manual', 'auto']:\n",
        "            AutoClean.help()\n",
        "            raise ValueError('Invalid value for \"mode\" parameter.')\n",
        "        if self.duplicates not in [False, 'auto']:\n",
        "            raise ValueError('Invalid value for \"duplicates\" parameter.')\n",
        "        if self.missing_num not in [False, 'auto', 'knn', 'mean', 'median', 'most_frequent', 'delete']:\n",
        "            raise ValueError('Invalid value for \"missing_num\" parameter.')\n",
        "        if self.missing_categ not in [False, 'auto', 'knn', 'most_frequent', 'delete']:\n",
        "            raise ValueError('Invalid value for \"missing_categ\" parameter.')\n",
        "        if self.outliers not in [False, 'auto', 'winz', 'delete']:\n",
        "            raise ValueError('Invalid value for \"outliers\" parameter.')\n",
        "        if isinstance(self.encode_categ, list):\n",
        "            if len(self.encode_categ) > 2 and self.encode_categ[0] not in ['auto', 'onehot', 'label']:\n",
        "                raise ValueError('Invalid value for \"encode_categ\" parameter.')\n",
        "            if len(self.encode_categ) == 2:\n",
        "                if not isinstance(self.encode_categ[1], list):\n",
        "                    raise ValueError('Invalid value for \"encode_categ\" parameter.')\n",
        "        else:\n",
        "            if not self.encode_categ in ['auto', False]:\n",
        "                raise ValueError('Invalid value for \"encode_categ\" parameter.')\n",
        "        if not isinstance(self.outlier_param, int) and not isinstance(self.outlier_param, float):\n",
        "            raise ValueError('Invalid value for \"outlier_param\" parameter.')  \n",
        "        if self.extract_datetime not in [False, 'auto', 'D','M','Y','h','m','s']:\n",
        "            raise ValueError('Invalid value for \"extract_datetime\" parameter.')  \n",
        "        if not isinstance(verbose, bool):\n",
        "            raise ValueError('Invalid value for \"verbose\" parameter.')  \n",
        "        if not isinstance(logfile, bool):\n",
        "            raise ValueError('Invalid value for \"logfile\" parameter.')  \n",
        "\n",
        "        logger.info('Completed validation of input parameters')\n",
        "        return\n",
        "            \n",
        "    def _clean_data(self, df, input_data):\n",
        "        # function for starting the autoclean process\n",
        "        df = df.reset_index(drop=True)\n",
        "        df = Duplicates.handle(self, df)\n",
        "        df = MissingValues.handle(self, df)\n",
        "        df = Outliers.handle(self, df)    \n",
        "        df = Adjust.convert_datetime(self, df) \n",
        "        df = EncodeCateg.handle(self, df)     \n",
        "        df = Adjust.round_values(self, df, input_data)\n",
        "        return df \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=AutoClean(renault_cluster, mode='auto', duplicates=False, missing_num=False, missing_categ=False, encode_categ=False, extract_datetime=False, outliers=False, outlier_param=1.5, logfile=True, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5wdEH1BDduL",
        "outputId": "5c9441c1-bca6-4ca7-df79-cdc95e12321a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoClean process completed in 182.489535 seconds\n",
            "Logfile saved to: /content/drive/MyDrive/Challenge Business & AI/B&AI/EDA/autoclean.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=y._clean_data(renault_cluster, None)"
      ],
      "metadata": {
        "id": "X658TYh2J7Nz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "VMwoZOctKbk-",
        "outputId": "5a704e7b-a344-493d-f8cf-010b60b164ec"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Unnamed: 0                       _id  \\\n",
              "0          0  62d2c4c482e3a449f212bca0   \n",
              "1          1  62d2c4b482e3a449f212bc9e   \n",
              "2          2  62d2c49182e3a449f212bb80   \n",
              "3          3  62d2c49182e3a449f212bb3f   \n",
              "4          4  62d2c49082e3a449f212bb3e   \n",
              "\n",
              "                                            ad_title car_brand car_model  \\\n",
              "0                       Renault Clio Limited 0.9 TCE   renault      clio   \n",
              "1  Renault Clio IV Limited 0.9 TCe 90 eco LED, NA...   renault      clio   \n",
              "2       Renault Clio 1.0 Experience SHZ Tempomat DAB   renault      clio   \n",
              "3       Renault Clio 1.0 Experience SHZ Tempomat DAB   renault      clio   \n",
              "4  Renault Clio 75 IV 1.2 16V Limited PDC Temp. R...   renault      clio   \n",
              "\n",
              "  ad_price car_reg_year    car_registration_year car_km car_energy  ...  \\\n",
              "0    12460       2020.0  2020-01-01 00:00:00.000  69802    essence  ...   \n",
              "1    11990       2017.0  2017-01-01 00:00:00.000  42000    essence  ...   \n",
              "2    12980       2019.0  2019-01-01 00:00:00.000  48932    essence  ...   \n",
              "3    12980       2019.0  2019-01-01 00:00:00.000  48932    essence  ...   \n",
              "4    12380       2017.0  2017-01-01 00:00:00.000  30124    essence  ...   \n",
              "\n",
              "  data_type_FULL data_type_de data_type_pro car_model_lab country_code_lab  \\\n",
              "0              0            0             0            14                7   \n",
              "1              0            0             0            14                7   \n",
              "2              0            0             0            14                7   \n",
              "3              0            0             0            14                7   \n",
              "4              0            0             0            14                7   \n",
              "\n",
              "  ad_inactive_EUR ad_inactive_False ad_inactive_True ad_inactive_reezocar.bot  \\\n",
              "0               0                 1                0                        0   \n",
              "1               0                 1                0                        0   \n",
              "2               0                 1                0                        0   \n",
              "3               0                 1                0                        0   \n",
              "4               0                 1                0                        0   \n",
              "\n",
              "  ad_inactive_scrap  \n",
              "0                 0  \n",
              "1                 0  \n",
              "2                 0  \n",
              "3                 0  \n",
              "4                 0  \n",
              "\n",
              "[5 rows x 41 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6761852-7fd0-4ce3-b362-ec45560947b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>_id</th>\n",
              "      <th>ad_title</th>\n",
              "      <th>car_brand</th>\n",
              "      <th>car_model</th>\n",
              "      <th>ad_price</th>\n",
              "      <th>car_reg_year</th>\n",
              "      <th>car_registration_year</th>\n",
              "      <th>car_km</th>\n",
              "      <th>car_energy</th>\n",
              "      <th>...</th>\n",
              "      <th>data_type_FULL</th>\n",
              "      <th>data_type_de</th>\n",
              "      <th>data_type_pro</th>\n",
              "      <th>car_model_lab</th>\n",
              "      <th>country_code_lab</th>\n",
              "      <th>ad_inactive_EUR</th>\n",
              "      <th>ad_inactive_False</th>\n",
              "      <th>ad_inactive_True</th>\n",
              "      <th>ad_inactive_reezocar.bot</th>\n",
              "      <th>ad_inactive_scrap</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>62d2c4c482e3a449f212bca0</td>\n",
              "      <td>Renault Clio Limited 0.9 TCE</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>12460</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>2020-01-01 00:00:00.000</td>\n",
              "      <td>69802</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>62d2c4b482e3a449f212bc9e</td>\n",
              "      <td>Renault Clio IV Limited 0.9 TCe 90 eco LED, NA...</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>11990</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>2017-01-01 00:00:00.000</td>\n",
              "      <td>42000</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>62d2c49182e3a449f212bb80</td>\n",
              "      <td>Renault Clio 1.0 Experience SHZ Tempomat DAB</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>12980</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>2019-01-01 00:00:00.000</td>\n",
              "      <td>48932</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>62d2c49182e3a449f212bb3f</td>\n",
              "      <td>Renault Clio 1.0 Experience SHZ Tempomat DAB</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>12980</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>2019-01-01 00:00:00.000</td>\n",
              "      <td>48932</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>62d2c49082e3a449f212bb3e</td>\n",
              "      <td>Renault Clio 75 IV 1.2 16V Limited PDC Temp. R...</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>12380</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>2017-01-01 00:00:00.000</td>\n",
              "      <td>30124</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 41 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6761852-7fd0-4ce3-b362-ec45560947b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b6761852-7fd0-4ce3-b362-ec45560947b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b6761852-7fd0-4ce3-b362-ec45560947b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "dT7-DV12NL0M",
        "outputId": "9d04608d-a17c-49d1-e86a-6a8a109f9f0f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                       _id  \\\n",
              "75848      75848  60980a3f7d3e012193cdc20e   \n",
              "75849      75849  60980a3f7d3e012193cdc210   \n",
              "75850      75850  60980a3f7d3e012193cdc214   \n",
              "75851      75851  60980a3f7d3e012193cdc21d   \n",
              "75852      75852  60980a3f7d3e012193cdc220   \n",
              "\n",
              "                                                ad_title car_brand car_model  \\\n",
              "75848                 renault clio 1.2 16v 75ch trend 5p   renault      clio   \n",
              "75849                 renault clio 1.2 16v 75ch trend 5p   renault      clio   \n",
              "75850  renault clio 0.9 tce 75ch energy limited 5p eu...   renault      clio   \n",
              "75851         renault clio iv 0.9 tce 90ch energy zen 5p   renault      clio   \n",
              "75852                     renault clio iv tce 90 limited   renault      clio   \n",
              "\n",
              "      ad_price car_reg_year    car_registration_year car_km car_energy  ...  \\\n",
              "75848     9480       2013.0  2021-06-25 00:00:00.000  40706    essence  ...   \n",
              "75849     8980       2013.0  2021-06-25 00:00:00.000  41059    essence  ...   \n",
              "75850    11290       2013.0  2021-06-25 00:00:00.000  26931    essence  ...   \n",
              "75851    10790       2013.0  2021-06-25 00:00:00.000  14612    essence  ...   \n",
              "75852     9990       2013.0  2021-06-25 00:00:00.000  54544    essence  ...   \n",
              "\n",
              "      data_type_FULL data_type_de data_type_pro car_model_lab  \\\n",
              "75848              1            0             0            14   \n",
              "75849              1            0             0            14   \n",
              "75850              1            0             0            14   \n",
              "75851              1            0             0            14   \n",
              "75852              1            0             0            14   \n",
              "\n",
              "      country_code_lab ad_inactive_EUR ad_inactive_False ad_inactive_True  \\\n",
              "75848                2               0                 0                1   \n",
              "75849                2               0                 0                1   \n",
              "75850                2               0                 0                1   \n",
              "75851                2               0                 0                1   \n",
              "75852                2               0                 0                1   \n",
              "\n",
              "      ad_inactive_reezocar.bot ad_inactive_scrap  \n",
              "75848                        0                 0  \n",
              "75849                        0                 0  \n",
              "75850                        0                 0  \n",
              "75851                        0                 0  \n",
              "75852                        0                 0  \n",
              "\n",
              "[5 rows x 41 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d189b32c-cf13-4014-b631-61531e7a5c1d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>_id</th>\n",
              "      <th>ad_title</th>\n",
              "      <th>car_brand</th>\n",
              "      <th>car_model</th>\n",
              "      <th>ad_price</th>\n",
              "      <th>car_reg_year</th>\n",
              "      <th>car_registration_year</th>\n",
              "      <th>car_km</th>\n",
              "      <th>car_energy</th>\n",
              "      <th>...</th>\n",
              "      <th>data_type_FULL</th>\n",
              "      <th>data_type_de</th>\n",
              "      <th>data_type_pro</th>\n",
              "      <th>car_model_lab</th>\n",
              "      <th>country_code_lab</th>\n",
              "      <th>ad_inactive_EUR</th>\n",
              "      <th>ad_inactive_False</th>\n",
              "      <th>ad_inactive_True</th>\n",
              "      <th>ad_inactive_reezocar.bot</th>\n",
              "      <th>ad_inactive_scrap</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>75848</th>\n",
              "      <td>75848</td>\n",
              "      <td>60980a3f7d3e012193cdc20e</td>\n",
              "      <td>renault clio 1.2 16v 75ch trend 5p</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>9480</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>2021-06-25 00:00:00.000</td>\n",
              "      <td>40706</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75849</th>\n",
              "      <td>75849</td>\n",
              "      <td>60980a3f7d3e012193cdc210</td>\n",
              "      <td>renault clio 1.2 16v 75ch trend 5p</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>8980</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>2021-06-25 00:00:00.000</td>\n",
              "      <td>41059</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75850</th>\n",
              "      <td>75850</td>\n",
              "      <td>60980a3f7d3e012193cdc214</td>\n",
              "      <td>renault clio 0.9 tce 75ch energy limited 5p eu...</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>11290</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>2021-06-25 00:00:00.000</td>\n",
              "      <td>26931</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75851</th>\n",
              "      <td>75851</td>\n",
              "      <td>60980a3f7d3e012193cdc21d</td>\n",
              "      <td>renault clio iv 0.9 tce 90ch energy zen 5p</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>10790</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>2021-06-25 00:00:00.000</td>\n",
              "      <td>14612</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75852</th>\n",
              "      <td>75852</td>\n",
              "      <td>60980a3f7d3e012193cdc220</td>\n",
              "      <td>renault clio iv tce 90 limited</td>\n",
              "      <td>renault</td>\n",
              "      <td>clio</td>\n",
              "      <td>9990</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>2021-06-25 00:00:00.000</td>\n",
              "      <td>54544</td>\n",
              "      <td>essence</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 41 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d189b32c-cf13-4014-b631-61531e7a5c1d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d189b32c-cf13-4014-b631-61531e7a5c1d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d189b32c-cf13-4014-b631-61531e7a5c1d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}